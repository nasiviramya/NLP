{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:21:47.054871Z","iopub.execute_input":"2025-03-24T10:21:47.055361Z","iopub.status.idle":"2025-03-24T10:21:51.110605Z","shell.execute_reply.started":"2025-03-24T10:21:47.055334Z","shell.execute_reply":"2025-03-24T10:21:51.109349Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"corpus = ''' Hi,this is ramya and iam learning about Tokenization.\niam trying to run my code in kaggale'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:21:51.112862Z","iopub.execute_input":"2025-03-24T10:21:51.113197Z","iopub.status.idle":"2025-03-24T10:21:51.117242Z","shell.execute_reply.started":"2025-03-24T10:21:51.113168Z","shell.execute_reply":"2025-03-24T10:21:51.116232Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:21:51.118414Z","iopub.execute_input":"2025-03-24T10:21:51.118755Z","iopub.status.idle":"2025-03-24T10:21:51.140106Z","shell.execute_reply.started":"2025-03-24T10:21:51.118729Z","shell.execute_reply":"2025-03-24T10:21:51.139018Z"}},"outputs":[{"name":"stdout","text":" Hi,this is ramya and iam learning about Tokenization.\niam trying to run my code in kaggale\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:21:51.141126Z","iopub.execute_input":"2025-03-24T10:21:51.141419Z","iopub.status.idle":"2025-03-24T10:21:51.158936Z","shell.execute_reply.started":"2025-03-24T10:21:51.141388Z","shell.execute_reply":"2025-03-24T10:21:51.157801Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# TOKEINIZATION : PARA -->> TO -->>SENTENCES\ndoc = sent_tokenize(corpus)\nprint(doc)\nprint(type(doc))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:21:51.160058Z","iopub.execute_input":"2025-03-24T10:21:51.160357Z","iopub.status.idle":"2025-03-24T10:21:51.177415Z","shell.execute_reply.started":"2025-03-24T10:21:51.160332Z","shell.execute_reply":"2025-03-24T10:21:51.176420Z"}},"outputs":[{"name":"stdout","text":"[' Hi,this is ramya and iam learning about Tokenization.', 'iam trying to run my code in kaggale']\n<class 'list'>\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"for sen in doc:\n    print(sen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:22:25.877231Z","iopub.execute_input":"2025-03-24T10:22:25.877631Z","iopub.status.idle":"2025-03-24T10:22:25.882691Z","shell.execute_reply.started":"2025-03-24T10:22:25.877596Z","shell.execute_reply":"2025-03-24T10:22:25.881666Z"}},"outputs":[{"name":"stdout","text":" Hi,this is ramya and iam learning about Tokenization.\niam trying to run my code in kaggale\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# TOKENIZATION\n#para--->>> words or sen ---> words\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:24:48.078455Z","iopub.execute_input":"2025-03-24T10:24:48.078976Z","iopub.status.idle":"2025-03-24T10:24:48.083337Z","shell.execute_reply.started":"2025-03-24T10:24:48.078943Z","shell.execute_reply":"2025-03-24T10:24:48.082126Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"word_tokenize(corpus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:24:51.573200Z","iopub.execute_input":"2025-03-24T10:24:51.573602Z","iopub.status.idle":"2025-03-24T10:24:51.580611Z","shell.execute_reply.started":"2025-03-24T10:24:51.573563Z","shell.execute_reply":"2025-03-24T10:24:51.579672Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['Hi',\n ',',\n 'this',\n 'is',\n 'ramya',\n 'and',\n 'iam',\n 'learning',\n 'about',\n 'Tokenization',\n '.',\n 'iam',\n 'trying',\n 'to',\n 'run',\n 'my',\n 'code',\n 'in',\n 'kaggale']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"for sen in doc:\n    print(word_tokenize(sen))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:26:53.078779Z","iopub.execute_input":"2025-03-24T10:26:53.079128Z","iopub.status.idle":"2025-03-24T10:26:53.085000Z","shell.execute_reply.started":"2025-03-24T10:26:53.079101Z","shell.execute_reply":"2025-03-24T10:26:53.083560Z"}},"outputs":[{"name":"stdout","text":"['Hi', ',', 'this', 'is', 'ramya', 'and', 'iam', 'learning', 'about', 'Tokenization', '.']\n['iam', 'trying', 'to', 'run', 'my', 'code', 'in', 'kaggale']\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from nltk.tokenize import wordpunct_tokenize\nwordpunct_tokenize(corpus)\n\n# ''' also splitted in it ex:it's it splitted into 3 tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:28:30.576619Z","iopub.execute_input":"2025-03-24T10:28:30.577093Z","iopub.status.idle":"2025-03-24T10:28:30.583982Z","shell.execute_reply.started":"2025-03-24T10:28:30.577058Z","shell.execute_reply":"2025-03-24T10:28:30.582998Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['Hi',\n ',',\n 'this',\n 'is',\n 'ramya',\n 'and',\n 'iam',\n 'learning',\n 'about',\n 'Tokenization',\n '.',\n 'iam',\n 'trying',\n 'to',\n 'run',\n 'my',\n 'code',\n 'in',\n 'kaggale']"},"metadata":{}}],"execution_count":22}]}